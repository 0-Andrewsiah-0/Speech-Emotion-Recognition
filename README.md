# Speech-Emotion-Recognition
  The Speech Emotion Recognition (SER) study aims to identify and interpret human emotions from audio language, as well as enhance the customer service interactions. Beside ,the goal this research is to improve customer interactions through accurate emotion recognition. The project has several critical components, including data collection, exploratory data analysis, data augmentation, and feature extraction and data modelling as well as deployment. A robust dataset is generated using data augmentation techniques such as adding noise, pitch shifting, time stretching, shifting, and changing volume. Mel-frequency cepstral coefficients (MFCCs) are used to extract essential audio elements. The study will investigate the impact of MFCC extraction by comparing model performance with and without the MFCC features.

   Three models ,CNN1D, LSTM, and GRU were compared to assess their effectiveness in speech emotion recognition. The base GRU model was chosen as the best-performing model, offering a strong balance of accuracy and generalizability with 91% accuracy. A key strength of this research is the experiment evaluating MFCC feature extraction, adding depth to the analysis and distinguishing it from prior work. By integrating detailed model comparisons and advanced feature extraction, this study not only improves model accuracy but also provides valuable insights for optimizing SER systems in real-world applications like call centres.
